{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t0.8148024746671689\n",
      "  (0, 2)\t0.5797386715376657\n",
      "  (1, 2)\t0.4494364165239821\n",
      "  (1, 1)\t0.6316672017376245\n",
      "  (1, 0)\t0.6316672017376245\n",
      "[[0.         0.         0.57973867 0.81480247]\n",
      " [0.6316672  0.6316672  0.44943642 0.        ]]\n",
      "{'have': 2, 'pen': 3, 'an': 0, 'apple': 1}\n",
      "['an', 'apple', 'have', 'pen']\n",
      "(2, 4)\n",
      "-------这里输出第 0 类文本的词语tf-idf权重------\n",
      "-------这里输出第 1 类文本的词语tf-idf权重------\n"
     ]
    }
   ],
   "source": [
    "#coding=utf-8\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "document = [\"I have a pen.\",\n",
    "            \"I have an apple.\"]\n",
    "tfidf_model = TfidfVectorizer().fit(document)\n",
    "weight=sparse_result = tfidf_model.transform(document)     # 得到tf-idf矩阵，稀疏矩阵表示法\n",
    "word=tfidf_model.get_feature_names()\n",
    "\n",
    "print(sparse_result)\n",
    "# (0, 3)\t0.814802474667\n",
    "# (0, 2)\t0.579738671538\n",
    "# (1, 2)\t0.449436416524\n",
    "# (1, 1)\t0.631667201738\n",
    "# (1, 0)\t0.631667201738\n",
    "print(sparse_result.todense())                     # 转化为更直观的一般矩阵\n",
    "# [[ 0.          0.          0.57973867  0.81480247]\n",
    "#  [ 0.6316672   0.6316672   0.44943642  0.        ]]\n",
    "print(tfidf_model.vocabulary_)                      # 词语与列的对应关系\n",
    "# {'have': 2, 'pen': 3, 'an': 0, 'apple': 1}\n",
    "print(word)\n",
    "print(weight.shape)\n",
    "for i in range(0,weight.shape[0]):  \n",
    "# 打印每类文本的tf-idf词语权重，第一个for遍历所有文本，\n",
    "#第二个for便利某一类文本下的词语权重\n",
    "    print (\"-------这里输出第\", i, \"类文本的词语tf-idf权重------\")\n",
    "#     for j in range(0,len(word)):\n",
    "#         print(word[j],weight[i][j])#第i个文本中，第j个次的tfidf值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一步：分词\n",
    "#### 中文不比英文，词语之间有着空格的自然分割，所以我们首先要进行分词处理，再把它转化为与上面的document类似的格式。这里采用著名的中文分词库jieba进行分词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我 是 一条 天狗 呀 ！', '我 把 月 来 吞 了 ，', '我 把 日来 吞 了 ，', '我 把 一切 的 星球 来 吞 了 ，', '我 把 全宇宙 来 吞 了 。', '我 便是 我 了 ！']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "text = \"\"\"我是一条天狗呀！\n",
    "我把月来吞了，\n",
    "我把日来吞了，\n",
    "我把一切的星球来吞了，\n",
    "我把全宇宙来吞了。\n",
    "我便是我了！\"\"\"\n",
    "sentences = text.split()\n",
    "sent_words = [list(jieba.cut(sent0)) for sent0 in sentences]\n",
    "document = [\" \".join(sent0) for sent0 in sent_words]\n",
    "print(document)\n",
    "# ['我 是 一条 天狗 呀 ！', '我 把 月 来 吞 了 ，', '我 把 日来 吞 了 ，', '我 把 一切 的 星球 来 吞 了 ，', '我 把 全宇宙 来 吞 了 。', '我 便是 我 了 ！']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PS：语料来自郭沫若《天狗》。另外，由于分词工具的不完善，也会有一些错误，比如这边错误地把\"日来\"分到了一起。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第二步：建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'一条': 1, '天狗': 4, '日来': 5, '一切': 0, '星球': 6, '全宇宙': 3, '便是': 2}\n",
      "  (0, 4)\t0.7071067811865476\n",
      "  (0, 1)\t0.7071067811865476\n",
      "  (2, 5)\t1.0\n",
      "  (3, 6)\t0.7071067811865476\n",
      "  (3, 0)\t0.7071067811865476\n",
      "  (4, 3)\t1.0\n",
      "  (5, 2)\t1.0\n",
      "[[0.         0.70710678 0.         0.         0.70710678 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         1.\n",
      "  0.        ]\n",
      " [0.70710678 0.         0.         0.         0.         0.\n",
      "  0.70710678]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         1.         0.         0.         0.\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# 理论上，现在得到的document的格式已经可以直接拿来训练了。让我们跑一下模型试试。\n",
    "tfidf_model = TfidfVectorizer().fit(document)\n",
    "print(tfidf_model.vocabulary_)\n",
    "# {'一条': 1, '天狗': 4, '日来': 5, '一切': 0, '星球': 6, '全宇宙': 3, '便是': 2}\n",
    "sparse_result = tfidf_model.transform(document)\n",
    "print(sparse_result)\n",
    "\n",
    "# (0, 4)\t0.707106781187\n",
    "# (0, 1)\t0.707106781187\n",
    "# (2, 5)\t1.0\n",
    "# (3, 6)\t0.707106781187\n",
    "# (3, 0)\t0.707106781187\n",
    "# (4, 3)\t1.0\n",
    "# (5, 2)\t1.0\n",
    "print(sparse_result.todense()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 没有错误，但有一个小问题，就是单字的词语，如“我”、“吞”、“呀”等词语在我们的词汇表中怎么都不见了呢？为了处理一些特殊的问题，让我们深入其中的一些参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第三步：参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 查了一些资料以后，发现单字的问题是token_pattern这个参数搞的鬼。它的默认值只匹配长度≥2的单词，就像其实开头的例子中的'I'也被忽略了一样，一般来说，长度为1的单词在英文中一般是无足轻重的，但在中文里，就可能有一些很重要的单字词，所以修改如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'我': 8, '是': 12, '一条': 1, '天狗': 7, '呀': 6, '把': 9, '月': 13, '来': 14, '吞': 5, '了': 2, '日来': 10, '一切': 0, '的': 15, '星球': 11, '全宇宙': 4, '便是': 3}\n"
     ]
    }
   ],
   "source": [
    "tfidf_model2 = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\").fit(document)\n",
    "print(tfidf_model2.vocabulary_)\n",
    "# {'我': 8, '是': 12, '一条': 1, '天狗': 7, '呀': 6, '把': 9, '月': 13, '来': 14, '吞': 5, '了': 2, '日来': 10, '一切': 0, '的': 15, '星球': 11, '全宇宙': 4, '便是': 3}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   token_pattern  这个参数使用正则表达式来分词，其默认参数为r\"(?u)\\b\\w\\w+\\b\"，其中的两个\\w决定了其匹配长度至少为2的单词，所以这边减到1个。对这个参数进行更多修改，可以满足其他要求，比如这里依然没有得到标点符号，在此不详解了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 当然有些时候我们还是要过滤掉一些无意义的词，下面有些别的参数也可以帮助我们实现这一目的：\n",
    "#### 1. max_df   和  min_df: [0.0, 1.0]内浮点数或正整数, 默认值=1.0\n",
    "#### 当设置为浮点数时，过滤出现在超过max_df 低于min_df比例的句子中的词语；正整数时,则是超过max_df句句子。\n",
    "#### 这样就可以帮助我们过滤掉出现太多的无意义词语，如下面的\"我\"就被过滤（虽然这里“我”的排比在文学上是很重要的）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'是': 8, '一条': 1, '天狗': 5, '呀': 4, '月': 9, '来': 10, '日来': 6, '一切': 0, '的': 11, '星球': 7, '全宇宙': 3, '便是': 2}\n"
     ]
    }
   ],
   "source": [
    "# 过滤出现在超过60%的句子中的词语\n",
    "tfidf_model3 = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\", max_df=0.6).fit(document)  \n",
    "print(tfidf_model3.vocabulary_)\n",
    "# {'是': 8, '一条': 1, '天狗': 5, '呀': 4, '月': 9, '来': 10, '日来': 6, '一切': 0, '的': 11, '星球': 7, '全宇宙': 3, '便是': 2}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.stop_words: list类型  直接过滤指定的停用词。       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'一条': 1, '天狗': 5, '呀': 4, '月': 8, '来': 9, '日来': 6, '一切': 0, '星球': 7, '全宇宙': 3, '便是': 2}\n"
     ]
    }
   ],
   "source": [
    "tfidf_model4 = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\", max_df=0.6, stop_words=[\"是\", \"的\"]).fit(document)\n",
    "print(tfidf_model4.vocabulary_)\n",
    "    # {'一条': 1, '天狗': 5, '呀': 4, '月': 8, '来': 9, '日来': 6, '一切': 0, '星球': 7, '全宇宙': 3, '便是': 2}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.vocabulary: dict类型\n",
    "#### 只使用特定的词汇，其形式与上面看到的tfidf_model4.vocabulary_相同，也是指定对应关系。\n",
    "#### 这一参数的使用有时能帮助我们专注于一些词语，比如我对本诗中表达感情的一些特定词语（甚至标点符号）感兴趣，就可以设定这一参数，只考虑他们：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'我': 0, '呀': 1, '!': 2}\n",
      "[[0.40572238 0.91399636 0.        ]\n",
      " [1.         0.         0.        ]\n",
      " [1.         0.         0.        ]\n",
      " [1.         0.         0.        ]\n",
      " [1.         0.         0.        ]\n",
      " [1.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_model5 = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\",vocabulary={\"我\":0, \"呀\":1,\"!\":2}).fit(document)\n",
    "print(tfidf_model5.vocabulary_)\n",
    "# {'我': 0, '呀': 1, '!': 2}\n",
    "print(tfidf_model5.transform(document).todense())\n",
    "# [[ 0.40572238  0.91399636  0.        ]\n",
    "#  [ 1.          0.          0.        ]\n",
    "#  [ 1.          0.          0.        ]\n",
    "#  [ 1.          0.          0.        ]\n",
    "#  [ 1.          0.          0.        ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn CountVectorizer  只考虑每种词汇在该训练文本中出现的频率，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
